{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYX0hFgfQ-Xj"
      },
      "source": [
        "Copyright 2022 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2WQtLKtiJBz"
      },
      "source": [
        "## Description\n",
        "\n",
        "This is a minimal implementation of a training data reconstruction attack with an informed adversary on MNIST, as described in [Balle et al. (2021)](https://arxiv.org/abs/2201.04845). \n",
        "\n",
        "The adversary creates a training set of (shadow model, shadow target) pairs, where each shadow model is trained on a fixed dataset and a shadow target. \n",
        "The reconstructor network is trained, taking as input (flattened) shadow model parameters, and outputing a reconstruction of the shadow target. The reconstructor network is then evaluated on a holdout set of (shadow model, shadow target) pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-AskZo5Mwbk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JbHbprFJayx"
      },
      "outputs": [],
      "source": [
        "!pip install dm-haiku\n",
        "!pip install optax\n",
        "\n",
        "import dataclasses\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "\n",
        "from sklearn import decomposition\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaGp-Yk0M3Ry"
      },
      "source": [
        "## Shadow model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZr9-7NNJer"
      },
      "source": [
        "### Shadow model training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2JBcyCfzg-t"
      },
      "outputs": [],
      "source": [
        "# Number of samples for shadow model evaluation.\n",
        "num_samples_mnist_prediction_eval=1000 #@param {type:\"integer\"}\n",
        "\n",
        "# Number of samples in fixed dataset.\n",
        "num_in_fixed=1000 #@param {type:\"integer\"}\n",
        "\n",
        "# Number of samples in reconstruction evaluation set.\n",
        "num_in_shadow_eval=1000 #@param {type:\"integer\"}\n",
        "\n",
        "# Number of shadow models used to train the reconstructor network. Making this \n",
        "# larger will improve reconstructions at the expense of longer training times.\n",
        "# Maximum value will be (70_000 - num_in_fixed - num_in_eval - \n",
        "# num_samples_mnist_prediction_eval).\n",
        "num_in_shadow_train=10000 #@param {type:\"integer\"}\n",
        "\n",
        "# MNIST data is shuffled using this seed. Used to create a consistent \n",
        "# shadow dataset.\n",
        "shadow_data_seed=42 #@param {type:\"integer\"}\n",
        "\n",
        "# Shadow models are all initialized with the same parameters using this seed.\n",
        "shadow_model_seed=42 #@param {type:\"integer\"}\n",
        "\n",
        "# Number of shadow model training epochs.\n",
        "num_shadow_model_epochs=100 #@param {type:\"integer\"}\n",
        "\n",
        "# Shadow model learning rate.\n",
        "shadow_model_lr=1e-1 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMfLDjD2NRpz"
      },
      "source": [
        "### Create data for shadow model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLV7Zo1MIgCW"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass(frozen=True)\n",
        "class ShadowDataset:\n",
        "  images: np.ndarray\n",
        "  labels: np.ndarray\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class ReconstructionDataset:\n",
        "  params: np.ndarray\n",
        "  images: np.ndarray\n",
        "\n",
        "def mnist_load_helper(split):\n",
        "  \"\"\"Loads a subset of the mnist dataset, reshapes, and normalizes images.\"\"\"\n",
        "  ds = tfds.load(\n",
        "      'mnist',\n",
        "      split=split,\n",
        "      batch_size=-1,\n",
        "      shuffle_files=False,\n",
        "      as_supervised=True,\n",
        "      with_info=False,\n",
        "      try_gcs=True)\n",
        "  ds_numpy = tfds.as_numpy(ds)\n",
        "  images, labels = ds_numpy\n",
        "  images_normalized = (images / 255).reshape([images.shape[0], 784])\n",
        "  return images_normalized, labels\n",
        "\n",
        "\n",
        "def load_data(\n",
        "    seed: int = 42,\n",
        "    num_samples_mnist_prediction_eval: int = 1_000,\n",
        "    num_in_fixed: int = 1_000,\n",
        "    num_in_shadow_eval: int = 1_000,\n",
        "    num_in_shadow_train: int = 10_000,\n",
        ") -\u003e Tuple[ShadowDataset, ShadowDataset, ShadowDataset, ShadowDataset]:\n",
        "  \"\"\"Data loading used for shadow model creation.\n",
        "\n",
        "  There is a constraint on num_samples_mnist_prediction_eval+ num_in_fixed +\n",
        "  num_in_shadow_eval + num_in_shadow_train. Specifically, the sum should be less\n",
        "  than or equal to the size of the MNIST dataset (70K) examples.\n",
        "\n",
        "  Args:\n",
        "    seed: (int) The seed used to shuffle the entire dataset before splitting.\n",
        "    num_samples_mnist_prediction_eval: (int) The number of samples on which each\n",
        "      shadow model is evaluated.\n",
        "    num_in_fixed: (int) The number of samples in the fixed dataset.\n",
        "    num_in_shadow_eval: (int) The number of targets that will be used to\n",
        "      evaluate the reconstructor network.\n",
        "    num_in_shadow_train: (int) The number of targets that will be used to train\n",
        "      the reconstructor network.\n",
        "\n",
        "  Returns:\n",
        "    A tuple (fixed_set, shadow_train, shadow_eval, test_set),\n",
        "    where each is a ShadowDataset with fields 'images' and 'labels' set.\n",
        "  \"\"\"\n",
        "  rng = np.random.default_rng(seed)\n",
        "\n",
        "  # Load all of the MNIST dataset and shuffle.\n",
        "  images_main, labels_main = mnist_load_helper('train+test')\n",
        "  idxs = rng.permutation(len(labels_main))\n",
        "  images_main = images_main[idxs]\n",
        "  labels_main = labels_main[idxs]\n",
        "\n",
        "  total_data_size = (\n",
        "      num_samples_mnist_prediction_eval + num_in_shadow_eval + num_in_fixed +\n",
        "      num_in_shadow_train)\n",
        "  if len(images_main) \u003c total_data_size:\n",
        "    raise ValueError(\n",
        "        'num_samples_mnist_prediction_eval + num_in_shadow_eval + num_in_fixed '\n",
        "        f'+ num_in_shadow_train is {total_data_size}, but should be '\n",
        "        f'less than {len(images_main)}.')\n",
        "\n",
        "  # Reserve first 'num_samples_mnist_prediction_eval' samples to evaluate\n",
        "  # shadow model. The following 'num_in_shadow_eval' samples are used to\n",
        "  # evaluate the reconstructor network. The next 'num_in_fixed' samples are\n",
        "  # assigned to the fixed dataset. The next 'num_in_shadow_train' will be\n",
        "  # targets for training the reconstructor network.\n",
        "  split_points = np.cumsum([\n",
        "      num_samples_mnist_prediction_eval,\n",
        "      num_in_shadow_eval,\n",
        "      num_in_fixed,\n",
        "      num_in_shadow_train,\n",
        "  ])\n",
        "  (mnist_eval_images, shadow_eval_images, images_fixed, shadow_train_images,\n",
        "   _) = np.split(images_main, split_points)\n",
        "  (mnist_eval_labels, shadow_eval_labels, labels_fixed, shadow_train_labels,\n",
        "   _) = np.split(labels_main, split_points)\n",
        "\n",
        "  mnist_eval = ShadowDataset(images=mnist_eval_images, labels=mnist_eval_labels)\n",
        "  fixed_set = ShadowDataset(images=images_fixed, labels=labels_fixed)\n",
        "  shadow_train_data = ShadowDataset(\n",
        "      images=shadow_train_images, labels=shadow_train_labels)\n",
        "  shadow_eval_data = ShadowDataset(\n",
        "      images=shadow_eval_images, labels=shadow_eval_labels)\n",
        "\n",
        "  return fixed_set, shadow_train_data, shadow_eval_data, mnist_eval\n",
        "\n",
        "\n",
        "fixed_set, shadow_train_data, shadow_eval_data, mnist_eval = load_data(\n",
        "    seed=shadow_data_seed,\n",
        "    num_in_fixed=num_in_fixed,\n",
        "    num_in_shadow_eval=num_in_shadow_eval,\n",
        "    num_in_shadow_train=num_in_shadow_train,\n",
        ")\n",
        "\n",
        "print('Number of shadow models for '\n",
        "      f'(reconstruction) training set: {len(shadow_train_data.images)}')\n",
        "print('Number of shadow models for '\n",
        "      f'(reconstruction) evaluation set: {len(shadow_eval_data.images)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDN-1H0fNVfF"
      },
      "source": [
        "### Train shadow models\n",
        "\n",
        "NB: With the default settings, expect an average shadow model test acc of ~0.85."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS_x6a5kv19Z"
      },
      "outputs": [],
      "source": [
        "def shadow_model_forward(images):\n",
        "  \"\"\"Shadow model architecture.\"\"\"\n",
        "  net = hk.nets.MLP([10, 10], activation=jax.nn.elu, activate_final=None)\n",
        "  return net(images)\n",
        "\n",
        "\n",
        "# Define shadow model and optimizer.\n",
        "shadow_model = hk.without_apply_rng(hk.transform(shadow_model_forward))\n",
        "opt_init, opt_update = optax.sgd(shadow_model_lr, momentum=0.9)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def xe_loss(params, images, labels):\n",
        "  \"\"\"Cross-entropy loss.\"\"\"\n",
        "  batch_size = images.shape[0]\n",
        "  logits = shadow_model.apply(params, images)\n",
        "  log_probs = jax.nn.log_softmax(logits)\n",
        "  return -jnp.sum(hk.one_hot(labels, 10) * log_probs) / batch_size\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def shadow_model_accuracy(params, images, labels):\n",
        "  \"\"\"Prediction accuracy.\"\"\"\n",
        "  predictions = shadow_model.apply(params, images)\n",
        "  return jnp.mean(jnp.argmax(predictions, axis=-1) == labels)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def shadow_model_update(params, opt_state, images_batch, labels_batch):\n",
        "  gradient = jax.grad(xe_loss)(params, images_batch, labels_batch)\n",
        "  updates, opt_state = opt_update(gradient, opt_state)\n",
        "  new_params = optax.apply_updates(params, updates)\n",
        "  return new_params, opt_state\n",
        "\n",
        "\n",
        "def shadow_model_train(step_fn, images_train, labels_train, images_test,\n",
        "                       labels_test, num_epochs):\n",
        "  rng = jax.random.PRNGKey(shadow_model_seed)\n",
        "  image = jnp.ones([1, len(images_train[-1])])\n",
        "  params = shadow_model.init(rng, image)\n",
        "  opt_state = opt_init(params)\n",
        "  for _ in range(num_epochs):\n",
        "    params, opt_state = step_fn(params, opt_state, images_train, labels_train)\n",
        "  test_acc = shadow_model_accuracy(params, images_test, labels_test)\n",
        "  return params, test_acc\n",
        "\n",
        "\n",
        "def create_shadow_model(images, labels, add_position):\n",
        "  \"\"\"Generates target and trains shadow model on fixed dataset + target.\"\"\"\n",
        "  add_position = jnp.minimum(add_position, len(labels) - 1)\n",
        "  image_add = jax.lax.dynamic_slice_in_dim(images, add_position, 1)\n",
        "  label_add = jax.lax.dynamic_slice_in_dim(labels, add_position, 1)\n",
        "  # Add additional target to training set.\n",
        "  images_train = jnp.concatenate([fixed_set.images, image_add])\n",
        "  labels_train = jnp.concatenate([fixed_set.labels, label_add])\n",
        "  # opt_params structure: [b_0 (1x10), w_0 (784x10), b_1 (1x10), w_1 (10x10)].\n",
        "  opt_params, test_acc = shadow_model_train(\n",
        "      step_fn=shadow_model_update,\n",
        "      images_train=images_train,\n",
        "      labels_train=labels_train,\n",
        "      images_test=mnist_eval.images,\n",
        "      labels_test=mnist_eval.labels,\n",
        "      num_epochs=num_shadow_model_epochs,\n",
        "  )\n",
        "  # jax.flatten_util.ravel_pytree flattens each leaf row-wise and then\n",
        "  # concatenates all leaves.\n",
        "  flat_params, _ = jax.flatten_util.ravel_pytree(opt_params)\n",
        "  return flat_params, image_add, test_acc\n",
        "\n",
        "\n",
        "def generate_shadow_data(num_shadow_models, images_in, labels_in, chunk_size):\n",
        "  \"\"\"Creates a shadow dataset of (parameter, target) pairs.\"\"\"\n",
        "  shadow_params = []\n",
        "  shadow_images = []\n",
        "  shadow_test_acc = []\n",
        "  vmap_fn = jax.vmap(create_shadow_model, in_axes=(None, None, 0))\n",
        "  for batch_idxs in tqdm.tqdm(\n",
        "      np.split(np.arange(num_shadow_models), num_shadow_models // chunk_size)):\n",
        "    # vmap create_shadow_model() over different targets.\n",
        "    params, images, test_acc = vmap_fn(images_in, labels_in, batch_idxs)\n",
        "    shadow_params.extend(params)\n",
        "    shadow_images.extend(images)\n",
        "    shadow_test_acc.extend(test_acc)\n",
        "  shadow_params = np.array(shadow_params)\n",
        "  shadow_images = np.array(shadow_images).squeeze(axis=1)\n",
        "  return shadow_params, shadow_images, shadow_test_acc\n",
        "\n",
        "\n",
        "# Generate shadow training data.\n",
        "train_chunk_size = 50\n",
        "(\n",
        "    shadow_train_params,\n",
        "    shadow_train_images,\n",
        "    shadow_train_test_acc,\n",
        ") = generate_shadow_data(num_in_shadow_train, shadow_train_data.images,\n",
        "                         shadow_train_data.labels, train_chunk_size)\n",
        "# Assign parameters of shadow models.\n",
        "np.testing.assert_allclose(shadow_train_images, shadow_train_data.images)\n",
        "\n",
        "# Generate shadow evaluation data.\n",
        "test_chunk_size = 10\n",
        "(\n",
        "    shadow_eval_params,\n",
        "    shadow_eval_images,\n",
        "    shadow_eval_test_acc,\n",
        ") = generate_shadow_data(num_in_shadow_eval, shadow_eval_data.images,\n",
        "                         shadow_eval_data.labels, test_chunk_size)\n",
        "# Assign parameters of shadow models.\n",
        "np.testing.assert_allclose(shadow_eval_images, shadow_eval_data.images)\n",
        "\n",
        "print('\\nAverage test accuracy of shadow (train) models: '\n",
        "      f'{np.mean(shadow_train_test_acc):.3f}')\n",
        "print('Average test accuracy of shadow (eval) models: '\n",
        "      f'{np.mean(shadow_eval_test_acc):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPA3y5OeNkab"
      },
      "source": [
        "### Pre-processing shadow model parameters for reconstruction\n",
        "\n",
        "If `fit_scaler_with_only_train=True` then the standard scaler constants are\n",
        "based only on shadow model training parameters. Reconstructions are usually\n",
        "better with `fit_scaler_with_only_train=False`, but if\n",
        "`num_in_shadow_eval` is large this causes some train / test leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHOs28ZjUNnX"
      },
      "outputs": [],
      "source": [
        "fit_scaler_with_only_train = True  #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "def scaler_fn(params_train, params_test, use_train_only=True):\n",
        "  if use_train_only:\n",
        "    scaler = preprocessing.StandardScaler().fit(params_train)\n",
        "  else:\n",
        "    scaler = preprocessing.StandardScaler().fit(\n",
        "        np.concatenate([params_train, params_test]))\n",
        "  params_train_scaled = scaler.transform(params_train)\n",
        "  params_test_scaled = scaler.transform(params_test)\n",
        "  return params_train_scaled, params_test_scaled\n",
        "\n",
        "\n",
        "shadow_train_rescaled_params, shadow_eval_rescaled_params = scaler_fn(\n",
        "    shadow_train_params,\n",
        "    shadow_eval_params,\n",
        "    use_train_only=fit_scaler_with_only_train,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ul5NsMsN1F5"
      },
      "source": [
        "### Visualise parameter space with PCA and create Reconstruction training and eval datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgZs9TYEJfJp"
      },
      "outputs": [],
      "source": [
        "# PCA on shadow model params\n",
        "pca = decomposition.PCA(n_components=2)\n",
        "pca.fit(np.concatenate([shadow_train_params, shadow_eval_params]))\n",
        "shadow_train_params_pca = pca.transform(shadow_train_params)\n",
        "shadow_eval_params_pca = pca.transform(shadow_eval_params)\n",
        "\n",
        "# PCA on rescaled shadow model params\n",
        "pca = decomposition.PCA(n_components=2)\n",
        "pca.fit(\n",
        "    np.concatenate([shadow_train_rescaled_params, shadow_eval_rescaled_params]))\n",
        "shadow_train_params_scaled_pca = pca.transform(shadow_train_rescaled_params)\n",
        "shadow_eval_params_scaled_pca = pca.transform(shadow_eval_rescaled_params)\n",
        "\n",
        "# Plot PCA\n",
        "fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=(8,3))\n",
        "ax1.scatter(\n",
        "    shadow_train_params_pca[:, 0],\n",
        "    shadow_train_params_pca[:, 1],\n",
        "    label='Shadow train',\n",
        "    alpha=.5)\n",
        "ax1.scatter(\n",
        "    shadow_eval_params_pca[:, 0],\n",
        "    shadow_eval_params_pca[:, 1],\n",
        "    label='Shadow test',\n",
        "    alpha=.5)\n",
        "ax1.set_title('PCA of params')\n",
        "ax1.legend(loc='best')\n",
        "\n",
        "ax2.scatter(\n",
        "    shadow_train_params_scaled_pca[:, 0],\n",
        "    shadow_train_params_scaled_pca[:, 1],\n",
        "    label='Shadow train',\n",
        "    alpha=.5)\n",
        "ax2.scatter(\n",
        "    shadow_eval_params_scaled_pca[:, 0],\n",
        "    shadow_eval_params_scaled_pca[:, 1],\n",
        "    label='Shadow test',\n",
        "    alpha=.5)\n",
        "ax2.set_title('PCA of params after pre-processing')\n",
        "ax2.legend(loc='best')\n",
        "\n",
        "# Create new training and eval dataset for reconstruction task.\n",
        "reconstruction_train_data = ReconstructionDataset(\n",
        "    params=shadow_train_rescaled_params, images=shadow_train_data.images)\n",
        "reconstruction_eval_data = ReconstructionDataset(\n",
        "    params=shadow_eval_rescaled_params, images=shadow_eval_data.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxBXRWuQOHHV"
      },
      "source": [
        "## Reconstruction attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ORQIBl1N-F2"
      },
      "source": [
        "### Train reconstruction attack\n",
        "\n",
        "NB: With the default settings, the test loss should be ~0.0670 after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_t6k6VbJ3Ya"
      },
      "outputs": [],
      "source": [
        "reconstructor_training_epochs = 200  #@param {type:\"integer\"}\n",
        "reconstructor_lr = 3e-4  #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "def reconstructor_network_forward(params):\n",
        "  \"\"\"Reconstructor network architecture.\"\"\"\n",
        "  net = hk.nets.MLP([1000, 1000, 784],\n",
        "                    activation=jax.nn.relu,\n",
        "                    activate_final=jax.nn.sigmoid)\n",
        "  return net(params)\n",
        "\n",
        "\n",
        "# Set-up reconstructor network and optimizer.\n",
        "reconstructor_network = hk.without_apply_rng(\n",
        "    hk.transform(reconstructor_network_forward))\n",
        "opt_init, opt_update = optax.rmsprop(reconstructor_lr)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def mse_loss(reconstructor_params, params_batch, images_batch):\n",
        "  \"\"\"MSE loss between reconstruction and target.\"\"\"\n",
        "  batch_size = params_batch.shape[0]\n",
        "  images_batch_pred = reconstructor_network.apply(reconstructor_params,\n",
        "                                                  params_batch)\n",
        "  return jnp.mean(jnp.mean((images_batch_pred - images_batch)**2, axis=1))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def mae_loss(reconstructor_params, params_batch, images_batch):\n",
        "  \"\"\"MAE loss between reconstruction and target.\"\"\"\n",
        "  batch_size = params_batch.shape[0]\n",
        "  images_batch_pred = reconstructor_network.apply(reconstructor_params,\n",
        "                                                  params_batch)\n",
        "  return jnp.mean(jnp.mean(jnp.abs(images_batch_pred - images_batch), axis=1))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def mse_and_mae_loss(reconstructor_params, params_batch, images_batch):\n",
        "  \"\"\"MSE and MAE loss between reconstruction and target.\"\"\"\n",
        "  mae = mae_loss(reconstructor_params, params_batch, images_batch)\n",
        "  mse = mse_loss(reconstructor_params, params_batch, images_batch)\n",
        "  return mae + mse, (mae, mse)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def reconstructor_network_update(reconstructor_params, opt_state, params_batch,\n",
        "                                 images_batch):\n",
        "  (loss, (_, _)), gradient = jax.value_and_grad(\n",
        "      mse_and_mae_loss, has_aux=True)(reconstructor_params, params_batch,\n",
        "                                      images_batch)\n",
        "  updates, opt_state = opt_update(gradient, opt_state)\n",
        "  new_reconstructor_params = optax.apply_updates(reconstructor_params, updates)\n",
        "  return new_reconstructor_params, opt_state, loss\n",
        "\n",
        "\n",
        "def reconstructor_network_train(step_fn, params_train, images_train,\n",
        "                                params_test, images_test, num_splits,\n",
        "                                num_epochs):\n",
        "  rng = jax.random.PRNGKey(42)\n",
        "  param = jnp.ones([1, params_train.shape[1]])\n",
        "  reconstructor_params = reconstructor_network.init(rng, param)\n",
        "  opt_state = opt_init(reconstructor_params)\n",
        "  print('Epoch\\tBatch\\tTrain_loss\\tTest_loss\\tTest_mae\\tTest_mse')\n",
        "  for epoch in range(num_epochs):\n",
        "    params_train, images_train = utils.shuffle(params_train, images_train)\n",
        "    batched_idxs = np.array_split(\n",
        "        np.arange(len(params_train)),\n",
        "        len(params_train) // num_splits)\n",
        "    for i, idxs in enumerate(batched_idxs):\n",
        "      reconstructor_params, opt_state, train_loss = step_fn(\n",
        "          reconstructor_params, opt_state, params_train[idxs],\n",
        "          images_train[idxs])\n",
        "      if i % 1000 == 0 and epoch % 10 == 0:\n",
        "        test_loss, (test_mae,\n",
        "                    test_mse) = mse_and_mae_loss(reconstructor_params,\n",
        "                                                 params_test, images_test)\n",
        "        print((f'{epoch}/{num_epochs}\\t{i}/{len(batched_idxs)}\\t'\n",
        "               f'{train_loss:\u003e8.4f}\\t{test_loss:\u003e8.4f}\\t{test_mae:\u003e8.4f}\\t'\n",
        "               f'{test_mse:\u003e8.4f}'))\n",
        "  return reconstructor_params\n",
        "\n",
        "\n",
        "# Train reconstructor network for 'reconstructor_training_epochs' epochs.\n",
        "opt_reconstructor_params = reconstructor_network_train(\n",
        "    step_fn=reconstructor_network_update,\n",
        "    params_train=reconstruction_train_data.params,\n",
        "    images_train=reconstruction_train_data.images,\n",
        "    params_test=reconstruction_eval_data.params,\n",
        "    images_test=reconstruction_eval_data.images,\n",
        "    num_splits=32,\n",
        "    num_epochs=reconstructor_training_epochs,\n",
        ")\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XqrW-rdOD5n"
      },
      "source": [
        "### Visualize training and evaluation target reconstructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU9aAXjKXtwR"
      },
      "outputs": [],
      "source": [
        "batch_size = 50  #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "\n",
        "# Helper function to display digit images\n",
        "def show_sample(images, sample_count=25, is_train=False):\n",
        "  # Create a square with can fit {sample_count} images.\n",
        "  grid_count = math.ceil(math.ceil(math.sqrt(sample_count)))\n",
        "  grid_count = min(grid_count, len(images))\n",
        "  plt.figure(figsize=(grid_count, grid_count))\n",
        "  for i in range(sample_count):\n",
        "    plt.subplot(grid_count, grid_count, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(images[i], cmap=plt.cm.gray, vmin=0, vmax=1.)\n",
        "  plt.suptitle(\n",
        "      'Shadow train targets' if is_train else 'Shadow eval targets',\n",
        "      fontsize=14)\n",
        "\n",
        "\n",
        "images_train_pred = reconstructor_network.apply(\n",
        "    opt_reconstructor_params, reconstruction_train_data.params[:batch_size])\n",
        "images_train_target = reconstruction_train_data.images[:batch_size]\n",
        "images_to_plot = np.empty(\n",
        "    (images_train_target.shape[0] + images_train_pred.shape[0],\n",
        "     images_train_target.shape[1]))\n",
        "images_to_plot[::2, :] = images_train_target\n",
        "images_to_plot[1::2, :] = images_train_pred\n",
        "images_to_plot = images_to_plot.reshape((images_to_plot.shape[0], 28, 28))\n",
        "show_sample(images_to_plot, sample_count=images_to_plot.shape[0], is_train=True)\n",
        "\n",
        "images_test_pred = reconstructor_network.apply(\n",
        "    opt_reconstructor_params, reconstruction_eval_data.params[:batch_size])\n",
        "images_test_target = reconstruction_eval_data.images[:batch_size]\n",
        "images_to_plot = np.empty(\n",
        "    (images_test_target.shape[0] + images_test_pred.shape[0],\n",
        "     images_test_target.shape[1]))\n",
        "images_to_plot[::2, :] = images_test_target\n",
        "images_to_plot[1::2, :] = images_test_pred\n",
        "images_to_plot = images_to_plot.reshape((images_to_plot.shape[0], 28, 28))\n",
        "show_sample(\n",
        "    images_to_plot, sample_count=images_to_plot.shape[0], is_train=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "F-AskZo5Mwbk",
        "KaGp-Yk0M3Ry",
        "oxBXRWuQOHHV"
      ],
      "name": "informed_adversary_mnist_reconstruction.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
